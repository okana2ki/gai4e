{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBzEfTwTPrSJsgSw1vEVM1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/okana2ki/gai4e/blob/main/emotion_sns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-genai gradio wordcloud matplotlib japanize-matplotlib\n",
        "\n",
        "import gradio as gr\n",
        "import threading, time\n",
        "from queue import Queue\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import japanize_matplotlib\n",
        "import json\n",
        "import google.genai as genai\n",
        "from google.colab import userdata\n",
        "from google.genai import types\n",
        "\n",
        "# --- API 初期化 ---\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "PRIMARY_MODEL = \"gemini-2.5-flash-lite\"\n",
        "FALLBACK_MODEL = \"gemini-2.0-flash-lite\"\n",
        "\n",
        "# --- 投稿キューと集計 ---\n",
        "llm_queue = Queue()\n",
        "processed_posts = []\n",
        "processed_count = 0\n",
        "queue_history = []\n",
        "\n",
        "# --- キュー可視化 ---\n",
        "def visualize_queue():\n",
        "    global llm_queue, processed_count\n",
        "    return {\"Queue\": llm_queue.qsize(), \"Processed\": processed_count}\n",
        "\n",
        "# --- LLM 呼び出し関数 ---\n",
        "def call_llm(post_text):\n",
        "    instruction = (\n",
        "        \"投稿テキストを受け取り、感情分析と重要ワード抽出をJSON形式で返してください。\"\n",
        "        \"形式: {\\\"sentiment\\\": \\\"joy\\\", \\\"keywords\\\": [\\\"word1\\\",\\\"word2\\\"]}\"\n",
        "    )\n",
        "    models_to_try = [PRIMARY_MODEL, FALLBACK_MODEL]\n",
        "\n",
        "    for model in models_to_try:\n",
        "        try:\n",
        "            response = client.models.generate_content(\n",
        "                model=model,\n",
        "                config=types.GenerateContentConfig(system_instruction=instruction),\n",
        "                contents=post_text\n",
        "            )\n",
        "            return json.loads(response.text)\n",
        "        except Exception as e:\n",
        "            print(f\"モデル {model} でエラー: {e}\")\n",
        "            continue\n",
        "\n",
        "    # どちらも失敗\n",
        "    return {\"sentiment\": \"error\", \"keywords\": []}\n",
        "\n",
        "# --- ワーカースレッド: 4秒ごとに1件処理 ---\n",
        "def llm_worker():\n",
        "    global processed_count, processed_posts\n",
        "    while True:\n",
        "        if not llm_queue.empty():\n",
        "            post = llm_queue.get()\n",
        "            result = call_llm(post)\n",
        "            processed_posts.append(result)\n",
        "            processed_count += 1\n",
        "        time.sleep(4)  # 15RPM 相当\n",
        "\n",
        "threading.Thread(target=llm_worker, daemon=True).start()\n",
        "\n",
        "# --- 集計グラフ作成 ---\n",
        "def generate_graphs():\n",
        "    # 感情集計\n",
        "    sentiments = [p[\"sentiment\"] for p in processed_posts if p[\"sentiment\"] != \"error\"]\n",
        "    sentiment_labels = [\"joy\", \"sadness\", \"anger\", \"fear\", \"neutral\", \"surprise\"]\n",
        "    counts = [sentiments.count(label) for label in sentiment_labels]\n",
        "\n",
        "    # 棒グラフ\n",
        "    fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
        "    ax[0].bar(sentiment_labels, counts, color='skyblue')\n",
        "    ax[0].set_title(\"感情分布\")\n",
        "\n",
        "    # ワードクラウド\n",
        "    all_words = []\n",
        "    for p in processed_posts:\n",
        "        all_words.extend(p.get(\"keywords\", []))\n",
        "    if all_words:\n",
        "        wc_text = \" \".join(all_words)\n",
        "        wc = WordCloud(font_path=\"/usr/share/fonts/truetype/NotoSansJP-Regular.otf\",\n",
        "                       width=600, height=400, background_color=\"white\").generate(wc_text)\n",
        "        ax[1].imshow(wc, interpolation=\"bilinear\")\n",
        "        ax[1].axis(\"off\")\n",
        "        ax[1].set_title(\"重要ワード\")\n",
        "    else:\n",
        "        ax[1].text(0.5,0.5,\"ワードなし\", ha=\"center\")\n",
        "        ax[1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# --- 投稿関数 ---\n",
        "def submit_post(post_text):\n",
        "    llm_queue.put(post_text)\n",
        "    return generate_graphs(), visualize_queue()\n",
        "\n",
        "# --- 表示更新ボタン ---\n",
        "def refresh_display():\n",
        "    return generate_graphs(), visualize_queue()\n",
        "\n",
        "# --- Gradio UI ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 高校生向け Gemini API レート制限デモ\")\n",
        "    with gr.Row():\n",
        "        input_text = gr.Textbox(label=\"投稿内容\")\n",
        "        submit_btn = gr.Button(\"投稿\")\n",
        "        refresh_btn = gr.Button(\"表示更新\")\n",
        "\n",
        "    output_graph = gr.Plot()\n",
        "    queue_status = gr.Label()\n",
        "\n",
        "    submit_btn.click(fn=submit_post, inputs=input_text, outputs=[output_graph, queue_status])\n",
        "    refresh_btn.click(fn=refresh_display, inputs=None, outputs=[output_graph, queue_status])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "zXdnQhX0w32I",
        "outputId": "1934b804-f157-4e82-965e-493560fb3adc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/4.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/4.1 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ed55ec6b77d320c6c9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ed55ec6b77d320c6c9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "import threading\n",
        "from queue import Queue\n",
        "import google.genai as genai\n",
        "from google.colab import userdata\n",
        "from google.genai import types\n",
        "\n",
        "# APIキーとクライアント初期化\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# モデルとフォールバック\n",
        "PRIMARY_MODEL = \"gemini-2.5-flash-lite\"\n",
        "FALLBACK_MODEL = \"gemini-2.0-flash-lite\"\n",
        "\n",
        "# 投稿キュー\n",
        "llm_queue = Queue()\n",
        "processed_count = 0\n",
        "queue_history = []\n",
        "\n",
        "# キュー可視化関数\n",
        "def visualize_queue():\n",
        "    global llm_queue, processed_count\n",
        "    return {\"Queue\": llm_queue.qsize(), \"Processed\": processed_count}\n",
        "\n",
        "# LLM 呼び出し関数（フォールバック対応）\n",
        "def call_llm(post_text):\n",
        "    global client\n",
        "    instruction = (\n",
        "        \"投稿テキストを受け取り、感情分析と重要ワード抽出をJSON形式で返してください。\"\n",
        "        \"形式: {\\\"sentiment\\\": \\\"joy\\\", \\\"keywords\\\": [\\\"word1\\\",\\\"word2\\\"]}\"\n",
        "    )\n",
        "\n",
        "    models_to_try = [PRIMARY_MODEL, FALLBACK_MODEL]\n",
        "\n",
        "    for model in models_to_try:\n",
        "        try:\n",
        "            response = client.models.generate_content(\n",
        "                model=model,\n",
        "                config=types.GenerateContentConfig(\n",
        "                    system_instruction=instruction\n",
        "                ),\n",
        "                contents=post_text\n",
        "            )\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            print(f\"モデル {model} でエラー: {e}\")\n",
        "            continue\n",
        "\n",
        "    # どちらのモデルでも失敗した場合\n",
        "    return '{\"sentiment\": \"error\", \"keywords\": []}'\n",
        "\n",
        "# ワーカースレッド: 4秒ごとに1件処理\n",
        "def llm_worker():\n",
        "    global processed_count\n",
        "    while True:\n",
        "        if not llm_queue.empty():\n",
        "            post = llm_queue.get()\n",
        "            print(f\"Processing: {post}\")\n",
        "            result = call_llm(post)  # 実際にAPI呼び出し\n",
        "            print(f\"Result: {result}\")\n",
        "            processed_count += 1\n",
        "        time.sleep(4)  # 15RPM相当\n",
        "\n",
        "threading.Thread(target=llm_worker, daemon=True).start()\n",
        "\n",
        "# 投稿ボタン\n",
        "def submit_post(post_text):\n",
        "    llm_queue.put(post_text)\n",
        "    return visualize_queue()\n",
        "\n",
        "# UI構築\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 高校生向け Gemini API レート制限デモ\")\n",
        "    with gr.Row():\n",
        "        input_text = gr.Textbox(label=\"投稿内容\")\n",
        "        submit_btn = gr.Button(\"投稿\")\n",
        "        refresh_btn = gr.Button(\"表示更新\")\n",
        "\n",
        "    queue_graph = gr.BarPlot(label=\"キューと処理状況\", x=[\"Queue\",\"Processed\"], y=[0,0], color=\"blue\")\n",
        "\n",
        "    submit_btn.click(fn=submit_post, inputs=input_text, outputs=queue_graph)\n",
        "    refresh_btn.click(fn=visualize_queue, inputs=None, outputs=queue_graph)\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "AavX-fNByX-1",
        "outputId": "81637fac-3661-424e-fa98-f770c0aad625"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3913e5a2c6f320b109.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3913e5a2c6f320b109.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-genai gradio plotly pandas wordcloud matplotlib\n",
        "!apt install -y fonts-noto-cjk\n",
        "\n",
        "import google.genai as genai\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "import json, re\n",
        "\n",
        "# 日本語フォントパス\n",
        "FONT_PATH = \"/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc\"\n",
        "\n",
        "# 投稿用DataFrame\n",
        "df = pd.DataFrame(columns=[\"投稿\", \"sentiment\", \"ai_comment\"])\n",
        "\n",
        "# --- LLM呼び出し＆JSON抽出 ---\n",
        "def analyze_post(text):\n",
        "    global df\n",
        "\n",
        "    system_instruction = (\n",
        "        \"あなたはSNS分析の専門家です。\"\n",
        "        \"以下の文章を解析して、JSON形式で出力してください。\"\n",
        "        \"JSONには2つのキーを含めます：\"\n",
        "        \"  1) 'sentiment' : joy, surprise, confusion, neutral のいずれかで文章の感情を分類してください。\"\n",
        "        \"  2) 'ai_comment' : 文章に対して短いポジティブなコメントを生成してください。\"\n",
        "        \"出力は必ずJSON形式のみで返してください。\"\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash-lite\",\n",
        "        config=genai.types.GenerateContentConfig(system_instruction=system_instruction),\n",
        "        contents=text\n",
        "    )\n",
        "\n",
        "    # デバッグ用：生レスポンス\n",
        "    print(\"Raw LLM response:\", response.text)\n",
        "\n",
        "    # JSON部分だけ抽出\n",
        "    match = re.search(r\"\\{.*\\}\", response.text, re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            result = json.loads(match.group())\n",
        "            sentiment = result.get(\"sentiment\", \"neutral\").strip().lower()\n",
        "            ai_comment = result.get(\"ai_comment\", \"\")\n",
        "        except:\n",
        "            sentiment = \"neutral\"\n",
        "            ai_comment = response.text\n",
        "    else:\n",
        "        sentiment = \"neutral\"\n",
        "        ai_comment = response.text\n",
        "\n",
        "    if sentiment not in [\"joy\",\"surprise\",\"confusion\",\"neutral\"]:\n",
        "        sentiment = \"neutral\"\n",
        "\n",
        "    # 新しい投稿を追加\n",
        "    df.loc[len(df)] = [text, sentiment, ai_comment]\n",
        "\n",
        "    # 集計更新\n",
        "    fig, wc_image = update_visualizations()\n",
        "    return fig, wc_image, ai_comment\n",
        "\n",
        "# --- 集計更新関数 ---\n",
        "def update_visualizations():\n",
        "    global df\n",
        "\n",
        "    # 棒グラフ\n",
        "    fig = px.histogram(\n",
        "        df,\n",
        "        x=\"sentiment\",\n",
        "        color=\"sentiment\",\n",
        "        text_auto=True,\n",
        "        category_orders={\"sentiment\":[\"joy\",\"surprise\",\"confusion\",\"neutral\"]},\n",
        "        title=\"オープンキャンパス感想のリアルタイム感情分析\",\n",
        "        color_discrete_map={\n",
        "            \"joy\":\"gold\",\n",
        "            \"surprise\":\"lightblue\",\n",
        "            \"confusion\":\"orange\",\n",
        "            \"neutral\":\"lightgrey\"\n",
        "        }\n",
        "    )\n",
        "    fig.update_layout(yaxis_title=\"投稿数\", xaxis_title=\"感情\")\n",
        "\n",
        "    # ワードクラウド\n",
        "    all_text = \" \".join(df[\"投稿\"].tolist())\n",
        "    wc = WordCloud(width=400, height=300, background_color=\"white\", font_path=FONT_PATH).generate(all_text)\n",
        "    wc_image = wc.to_image()  # PIL.Image形式\n",
        "\n",
        "    return fig, wc_image\n",
        "\n",
        "# --- Gradio UI ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# オープンキャンパス感想SNSデモ\")\n",
        "    gr.Markdown(\"投稿ごとにAIが感情分析を行い、グラフとワードクラウドが即座に更新されます。\")\n",
        "\n",
        "    input_text = gr.Textbox(label=\"あなたの感想を入力してください\", placeholder=\"例：楽しかった！\")\n",
        "    submit_btn = gr.Button(\"投稿\")\n",
        "    update_btn = gr.Button(\"集計更新\")\n",
        "\n",
        "    ai_comment_output = gr.Textbox(label=\"AIコメント\", interactive=False)\n",
        "    graph_output = gr.Plot(label=\"感情分析グラフ\")\n",
        "    wc_output = gr.Image(label=\"ワードクラウド\")\n",
        "\n",
        "    # 投稿ボタン → 新規投稿＋自動更新\n",
        "    submit_btn.click(\n",
        "        fn=analyze_post,\n",
        "        inputs=input_text,\n",
        "        outputs=[graph_output, wc_output, ai_comment_output]\n",
        "    )\n",
        "\n",
        "    # 更新ボタン → DataFrameは変更せず集計更新\n",
        "    update_btn.click(\n",
        "        fn=update_visualizations,\n",
        "        inputs=None,\n",
        "        outputs=[graph_output, wc_output]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "id": "oIlHoIDclVey",
        "outputId": "17d55863-18fe-4a86-d699-ff116d31c98a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-noto-cjk is already the newest version (1:20220127+repack1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://61f4df578d3466e75d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://61f4df578d3466e75d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-genai gradio plotly pandas wordcloud matplotlib\n",
        "!apt install -y fonts-noto-cjk\n",
        "\n",
        "import google.genai as genai\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "# 日本語フォントパス\n",
        "FONT_PATH = \"/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc\"\n",
        "\n",
        "# 投稿用DataFrame\n",
        "df = pd.DataFrame(columns=[\"投稿\", \"sentiment\", \"ai_comment\"])\n",
        "\n",
        "# --- 1回のLLM呼び出しでJSON取得 ---\n",
        "def analyze_post(text):\n",
        "    global df\n",
        "\n",
        "    system_instruction = (\n",
        "        \"あなたはSNS分析の専門家です。\"\n",
        "        \"以下の文章を解析して、JSON形式で出力してください。\"\n",
        "        \"JSONには2つのキーを含めます：\"\n",
        "        \"  1) 'sentiment' : joy, surprise, confusion, neutral のいずれかで文章の感情を分類してください。\"\n",
        "        \"  2) 'ai_comment' : 文章に対して短いポジティブなコメントを生成してください。\"\n",
        "        \"出力は必ずJSON形式のみで返してください。\"\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash-lite\",\n",
        "        config=genai.types.GenerateContentConfig(system_instruction=system_instruction),\n",
        "        contents=text\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        result = json.loads(response.text)\n",
        "        sentiment = result.get(\"sentiment\", \"neutral\")\n",
        "        ai_comment = result.get(\"ai_comment\", \"\")\n",
        "    except:\n",
        "        sentiment = \"neutral\"\n",
        "        ai_comment = response.text\n",
        "\n",
        "    if sentiment not in [\"joy\", \"surprise\", \"confusion\", \"neutral\"]:\n",
        "        sentiment = \"neutral\"\n",
        "\n",
        "    df.loc[len(df)] = [text, sentiment, ai_comment]\n",
        "\n",
        "    # 集計更新\n",
        "    fig, wc_image = update_visualizations()\n",
        "    return fig, wc_image, ai_comment\n",
        "\n",
        "# --- 集計更新 ---\n",
        "def update_visualizations():\n",
        "    global df\n",
        "\n",
        "    # 棒グラフ\n",
        "    fig = px.histogram(\n",
        "        df,\n",
        "        x=\"sentiment\",\n",
        "        color=\"sentiment\",\n",
        "        text_auto=True,\n",
        "        category_orders={\"sentiment\":[\"joy\",\"surprise\",\"confusion\",\"neutral\"]},\n",
        "        title=\"オープンキャンパス感想のリアルタイム感情分析\",\n",
        "        color_discrete_map={\n",
        "            \"joy\":\"gold\",\n",
        "            \"surprise\":\"lightblue\",\n",
        "            \"confusion\":\"orange\",\n",
        "            \"neutral\":\"lightgrey\"\n",
        "        }\n",
        "    )\n",
        "    fig.update_layout(yaxis_title=\"投稿数\", xaxis_title=\"感情\")\n",
        "\n",
        "    # ワードクラウド\n",
        "    all_text = \" \".join(df[\"投稿\"].tolist())\n",
        "    wc = WordCloud(width=400, height=300, background_color=\"white\", font_path=FONT_PATH).generate(all_text)\n",
        "    wc_image = wc.to_image()  # PIL.Image形式\n",
        "\n",
        "    return fig, wc_image\n",
        "\n",
        "# --- Gradio UI ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# オープンキャンパス感想SNSデモ（リアルタイム自動更新）\")\n",
        "    gr.Markdown(\"投稿ごとにAIが感情分析を行い、グラフとワードクラウドが即座に更新されます。\")\n",
        "\n",
        "    input_text = gr.Textbox(label=\"あなたの感想を入力してください\", placeholder=\"例：楽しかった！\")\n",
        "    submit_btn = gr.Button(\"投稿\")\n",
        "\n",
        "    ai_comment_output = gr.Textbox(label=\"AIコメント\", interactive=False)\n",
        "    graph_output = gr.Plot(label=\"感情分析グラフ\")\n",
        "    wc_output = gr.Image(label=\"ワードクラウド\")\n",
        "\n",
        "    # 投稿→解析＋自動更新（出力をflatten）\n",
        "    submit_btn.click(\n",
        "        fn=analyze_post,\n",
        "        inputs=input_text,\n",
        "        outputs=[graph_output, wc_output, ai_comment_output]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "id": "CKc0NewBicW3",
        "outputId": "276af53e-d94c-4dfb-cd66-0a571f081232"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  fonts-noto-cjk-extra\n",
            "The following NEW packages will be installed:\n",
            "  fonts-noto-cjk\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 61.2 MB of archives.\n",
            "After this operation, 93.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-cjk all 1:20220127+repack1-1 [61.2 MB]\n",
            "Fetched 61.2 MB in 1s (52.1 MB/s)\n",
            "Selecting previously unselected package fonts-noto-cjk.\n",
            "(Reading database ... 126380 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-noto-cjk_1%3a20220127+repack1-1_all.deb ...\n",
            "Unpacking fonts-noto-cjk (1:20220127+repack1-1) ...\n",
            "Setting up fonts-noto-cjk (1:20220127+repack1-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b4bd8aeaae67d87635.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b4bd8aeaae67d87635.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab向け\n",
        "!pip install -q -U google-genai gradio plotly pandas wordcloud matplotlib\n",
        "\n",
        "import google.genai as genai\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "import json\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# --- 日本語フォントを用意（Colab用） ---\n",
        "!wget -O /usr/share/fonts/truetype/NotoSansJP-Regular.otf https://noto-website-2.storage.googleapis.com/pkgs/NotoSansJP-unhinted.zip\n",
        "!unzip -o /usr/share/fonts/truetype/NotoSansJP-Regular.otf -d /usr/share/fonts/truetype/\n",
        "FONT_PATH = \"/usr/share/fonts/truetype/NotoSansJP-Regular.otf\"\n",
        "\n",
        "# --- 投稿用DataFrame ---\n",
        "df = pd.DataFrame(columns=[\"投稿\", \"sentiment\", \"ai_comment\"])\n",
        "\n",
        "# --- 1回のLLM呼び出しでJSON取得 ---\n",
        "def analyze_post(text):\n",
        "    global df\n",
        "\n",
        "    system_instruction = (\n",
        "        \"あなたはSNS分析の専門家です。\"\n",
        "        \"以下の文章を解析して、JSON形式で出力してください。\"\n",
        "        \"JSONには2つのキーを含めます：\"\n",
        "        \"  1) 'sentiment' : joy, surprise, confusion, neutral のいずれかで文章の感情を分類してください。\"\n",
        "        \"  2) 'ai_comment' : 文章に対して短いポジティブなコメントを生成してください。\"\n",
        "        \"出力は必ずJSON形式のみで返してください。\"\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash-lite\",\n",
        "        config=genai.types.GenerateContentConfig(system_instruction=system_instruction),\n",
        "        contents=text\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        result = json.loads(response.text)\n",
        "        sentiment = result.get(\"sentiment\", \"neutral\")\n",
        "        ai_comment = result.get(\"ai_comment\", \"\")\n",
        "    except:\n",
        "        sentiment = \"neutral\"\n",
        "        ai_comment = response.text\n",
        "\n",
        "    # 感情ラベルを統一\n",
        "    if sentiment not in [\"joy\", \"surprise\", \"confusion\", \"neutral\"]:\n",
        "        sentiment = \"neutral\"\n",
        "\n",
        "    # DataFrameに追加\n",
        "    df.loc[len(df)] = [text, sentiment, ai_comment]\n",
        "\n",
        "    # 投稿ごとに即座に集計更新\n",
        "    return update_visualizations(), ai_comment\n",
        "\n",
        "# --- 集計結果更新 ---\n",
        "def update_visualizations():\n",
        "    global df\n",
        "\n",
        "    # 棒グラフ（ラベル順序固定）\n",
        "    fig = px.histogram(\n",
        "        df,\n",
        "        x=\"sentiment\",\n",
        "        color=\"sentiment\",\n",
        "        text_auto=True,\n",
        "        category_orders={\"sentiment\":[\"joy\",\"surprise\",\"confusion\",\"neutral\"]},\n",
        "        title=\"オープンキャンパス感想のリアルタイム感情分析\",\n",
        "        color_discrete_map={\n",
        "            \"joy\":\"gold\",\n",
        "            \"surprise\":\"lightblue\",\n",
        "            \"confusion\":\"orange\",\n",
        "            \"neutral\":\"lightgrey\"\n",
        "        }\n",
        "    )\n",
        "    fig.update_layout(yaxis_title=\"投稿数\", xaxis_title=\"感情\")\n",
        "\n",
        "    # 日本語対応ワードクラウド\n",
        "    all_text = \" \".join(df[\"投稿\"].tolist())\n",
        "    wc = WordCloud(width=400, height=300, background_color=\"white\", font_path=FONT_PATH).generate(all_text)\n",
        "    wc_image = wc.to_image()  # PIL.Image形式に変換\n",
        "\n",
        "    return fig, wc_image\n",
        "\n",
        "# --- Gradio UI ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# オープンキャンパス感想SNSデモ（リアルタイム自動更新）\")\n",
        "    gr.Markdown(\"投稿ごとにAIが感情分析を行い、グラフとワードクラウドが即座に更新されます。\")\n",
        "\n",
        "    input_text = gr.Textbox(label=\"あなたの感想を入力してください\", placeholder=\"例：楽しかった！\")\n",
        "    submit_btn = gr.Button(\"投稿\")\n",
        "\n",
        "    ai_comment_output = gr.Textbox(label=\"AIコメント\", interactive=False)\n",
        "    graph_output = gr.Plot(label=\"感情分析グラフ\")\n",
        "    wc_output = gr.Image(label=\"ワードクラウド\")\n",
        "\n",
        "    # 投稿→解析＋自動更新\n",
        "    submit_btn.click(\n",
        "        fn=analyze_post,\n",
        "        inputs=input_text,\n",
        "        outputs=[(graph_output, wc_output), ai_comment_output]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "yhXR9f_VhchU",
        "outputId": "d4449914-60f3-4b23-e1f1-33d443abe503"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-17 14:54:13--  https://noto-website-2.storage.googleapis.com/pkgs/NotoSansJP-unhinted.zip\n",
            "Resolving noto-website-2.storage.googleapis.com (noto-website-2.storage.googleapis.com)... 108.177.121.207, 142.251.189.207, 142.250.125.207, ...\n",
            "Connecting to noto-website-2.storage.googleapis.com (noto-website-2.storage.googleapis.com)|108.177.121.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2025-08-17 14:54:13 ERROR 403: Forbidden.\n",
            "\n",
            "Archive:  /usr/share/fonts/truetype/NotoSansJP-Regular.otf\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /usr/share/fonts/truetype/NotoSansJP-Regular.otf or\n",
            "        /usr/share/fonts/truetype/NotoSansJP-Regular.otf.zip, and cannot find /usr/share/fonts/truetype/NotoSansJP-Regular.otf.ZIP, period.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute '_id'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4111018745.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# 投稿→解析＋自動更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     submit_btn.click(\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manalyze_post\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/events.py\u001b[0m in \u001b[0;36mevent_trigger\u001b[0;34m(block, fn, inputs, outputs, api_name, api_description, scroll_to_output, show_progress, show_progress_on, queue, batch, max_batch_size, preprocess, postprocess, cancels, trigger_mode, js, concurrency_limit, concurrency_id, show_api, time_limit, stream_every, like_user_message, key)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_callback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdep_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0mevent_trigger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_event_name\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m             \u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m             \u001b[0;34m\"backend_fn\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;34m\"js\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m             \u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m             \u001b[0;34m\"backend_fn\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;34m\"js\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute '_id'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab向け\n",
        "!pip install -q -U google-genai gradio plotly pandas wordcloud matplotlib\n",
        "\n",
        "import google.genai as genai\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import json\n",
        "\n",
        "# --- グローバル変数: 投稿を蓄積するDataFrame ---\n",
        "df = pd.DataFrame(columns=[\"投稿\", \"sentiment\", \"ai_comment\"])\n",
        "\n",
        "# --- LLMを1回だけ呼んで感情とAIコメントをJSONで取得 ---\n",
        "def analyze_post(text):\n",
        "    global df\n",
        "\n",
        "    system_instruction = (\n",
        "        \"あなたはSNS分析の専門家です。\"\n",
        "        \"以下の文章を解析して、JSON形式で出力してください。\"\n",
        "        \"JSONには2つのキーを含めます：\"\n",
        "        \"  1) 'sentiment' : joy, surprise, confusion, neutral のいずれかで文章の感情を分類してください。\"\n",
        "        \"  2) 'ai_comment' : 文章に対して短いポジティブなコメントを生成してください。\"\n",
        "        \"出力は必ずJSON形式のみで返してください。\"\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash-lite\",\n",
        "        config=genai.types.GenerateContentConfig(system_instruction=system_instruction),\n",
        "        contents=text\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        result = json.loads(response.text)\n",
        "        sentiment = result.get(\"sentiment\", \"neutral\")\n",
        "        ai_comment = result.get(\"ai_comment\", \"\")\n",
        "    except:\n",
        "        sentiment = \"neutral\"\n",
        "        ai_comment = response.text\n",
        "\n",
        "    # DataFrameに追加\n",
        "    df.loc[len(df)] = [text, sentiment, ai_comment]\n",
        "\n",
        "    return ai_comment  # AIコメントのみ返す\n",
        "\n",
        "# --- 集計結果の更新: 棒グラフとワードクラウド ---\n",
        "def update_visualizations():\n",
        "    global df\n",
        "\n",
        "    # 感情分布グラフ\n",
        "    fig = px.histogram(\n",
        "        df,\n",
        "        x=\"sentiment\",\n",
        "        color=\"sentiment\",\n",
        "        text_auto=True,\n",
        "        title=\"オープンキャンパス感想のリアルタイム感情分析\",\n",
        "        color_discrete_map={\n",
        "            \"joy\":\"gold\",\n",
        "            \"surprise\":\"lightblue\",\n",
        "            \"confusion\":\"orange\",\n",
        "            \"neutral\":\"lightgrey\"\n",
        "        }\n",
        "    )\n",
        "    fig.update_layout(yaxis_title=\"投稿数\", xaxis_title=\"感情\")\n",
        "\n",
        "    # ワードクラウド生成\n",
        "    all_text = \" \".join(df[\"投稿\"].tolist())\n",
        "    wc = WordCloud(width=400, height=300, background_color=\"white\").generate(all_text)\n",
        "    wc_image = wc.to_image()  # PIL.Image形式に変換\n",
        "\n",
        "    return fig, wc_image\n",
        "\n",
        "# --- Gradio UI ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# オープンキャンパス感想SNSデモ（リアルタイム更新）\")\n",
        "    gr.Markdown(\"投稿ごとにAIが感情分析を行い、最新集計を可視化します。\")\n",
        "\n",
        "    input_text = gr.Textbox(label=\"あなたの感想を入力してください\", placeholder=\"例：楽しかった！\")\n",
        "    submit_btn = gr.Button(\"投稿\")\n",
        "    update_btn = gr.Button(\"表示更新\")\n",
        "\n",
        "    ai_comment_output = gr.Textbox(label=\"AIコメント\", interactive=False)\n",
        "    graph_output = gr.Plot(label=\"感情分析グラフ\")\n",
        "    wc_output = gr.Image(label=\"ワードクラウド\")\n",
        "\n",
        "    # 投稿→LLM解析\n",
        "    submit_btn.click(analyze_post, inputs=input_text, outputs=ai_comment_output)\n",
        "\n",
        "    # 表示更新→最新集計描画\n",
        "    update_btn.click(update_visualizations, outputs=[graph_output, wc_output])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "xxvzCTqOfKOd",
        "outputId": "7d8638f8-5857-4bbc-835a-c2212b7ecb32"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://cbf898d7bfdbdeb6c9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cbf898d7bfdbdeb6c9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "id": "Vk28J5PVZate",
        "outputId": "3b380502-5a8c-4d7a-bf27-0a2502514dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.3/229.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://cce09694e6050f7979.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://cce09694e6050f7979.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install -q -U google-genai gradio plotly pandas wordcloud matplotlib\n",
        "\n",
        "import google.genai as genai\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from io import BytesIO\n",
        "import base64\n",
        "import json\n",
        "\n",
        "# データ保存用\n",
        "df = pd.DataFrame(columns=[\"投稿\", \"感情\", \"AIコメント\"])\n",
        "\n",
        "def analyze_and_comment_single_call(text):\n",
        "    global df\n",
        "\n",
        "    # 1回の呼び出しで感情とAIコメントをJSONで返す指示\n",
        "    system_instruction = (\n",
        "        \"あなたはSNS分析の専門家です。\"\n",
        "        \"以下の文章を解析して、JSON形式で出力してください。\"\n",
        "        \"JSONには2つのキーを含めます：\"\n",
        "        \"  1) 'sentiment' : joy, surprise, confusion, neutral のいずれかで文章の感情を分類してください。\"\n",
        "        \"  2) 'ai_comment' : 文章に対して短いポジティブなコメントを生成してください。\"\n",
        "        \"出力は必ずJSON形式のみで返してください。\"\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-2.5-flash-lite\",\n",
        "        config=genai.types.GenerateContentConfig(system_instruction=system_instruction),\n",
        "        contents=text\n",
        "    )\n",
        "\n",
        "    # GeminiからのテキストをJSONとしてパース\n",
        "    try:\n",
        "        result = json.loads(response.text)\n",
        "        sentiment = result.get(\"sentiment\", \"neutral\")\n",
        "        ai_comment = result.get(\"ai_comment\", \"\")\n",
        "    except Exception as e:\n",
        "        # パース失敗時はデフォルト\n",
        "        sentiment = \"neutral\"\n",
        "        ai_comment = response.text\n",
        "\n",
        "    # データ保存\n",
        "    df.loc[len(df)] = [text, sentiment, ai_comment]\n",
        "\n",
        "    # 感情分布グラフ\n",
        "    fig = px.histogram(df, x=\"感情\", color=\"感情\", text_auto=True,\n",
        "                       title=\"オープンキャンパス感想のリアルタイム感情分析\",\n",
        "                       color_discrete_map={\n",
        "                           \"joy\":\"gold\",\n",
        "                           \"surprise\":\"lightblue\",\n",
        "                           \"confusion\":\"orange\",\n",
        "                           \"neutral\":\"lightgrey\"\n",
        "                       })\n",
        "    fig.update_layout(yaxis_title=\"投稿数\", xaxis_title=\"感情\")\n",
        "\n",
        "    # ワードクラウド生成\n",
        "    all_text = \" \".join(df[\"投稿\"].tolist())\n",
        "    wc = WordCloud(width=400, height=300, background_color=\"white\").generate(all_text)\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    buf = BytesIO()\n",
        "    plt.savefig(buf, format=\"png\")\n",
        "    buf.seek(0)\n",
        "    img_data = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
        "    buf.close()\n",
        "\n",
        "    return fig, f\"AIコメント: {ai_comment}\", f\"ワードクラウド\", img_data\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# オープンキャンパス感想SNS（1回呼び出し版）\")\n",
        "    gr.Markdown(\"投稿するとAIが感情を分析してJSON形式で返し、可視化します。\")\n",
        "\n",
        "    input_text = gr.Textbox(label=\"あなたの感想を入力してください\", placeholder=\"例：楽しかった！\")\n",
        "    graph_output = gr.Plot(label=\"感情分析グラフ\")\n",
        "    ai_comment_output = gr.Textbox(label=\"AIコメント\", interactive=False)\n",
        "    wc_output = gr.Image(label=\"ワードクラウド\")\n",
        "\n",
        "    input_text.submit(analyze_and_comment_single_call,\n",
        "                      inputs=input_text,\n",
        "                      outputs=[graph_output, ai_comment_output, wc_output])\n",
        "\n",
        "demo.launch()\n"
      ]
    }
  ]
}